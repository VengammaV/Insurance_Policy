{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2724bfc3-4829-4a60-907d-385333fdeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d36c51-9162-4a2c-b0ca-a3c3cb030467",
   "metadata": {},
   "source": [
    "# Function to preprocess the English Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac58db0-a279-4c70-a33e-0c183c059c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra spaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Remove non-printable characters\n",
    "    text = re.sub(r'[^\\x20-\\x7E]', '', text)\n",
    "\n",
    "    # Optional: Remove special symbols (e.g., HTML tags, currency symbols)\n",
    "    text = re.sub(r'<.*?>', '', text)  # remove HTML\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?\\'\"-]', '', text)  # allow punctuation\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12997d9d-d157-48bb-ba19-8f7b56433950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"./Data/insurance_policies.xlsx\")\n",
    "df[\"Policy_Text_EN_Clean\"] = df[\"Policy_Text_EN\"].apply(preprocess_text)\n",
    "texts = df[\"Policy_Text_EN_Clean\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e402ff-a046-49d7-bf28-6f666bf0cf17",
   "metadata": {},
   "source": [
    "# For complete translations, implement the method:\n",
    "1. Splits long policy texts into sentences.\n",
    "2. Translates each sentence individually (to avoid token overflow).\n",
    "3. Reassembles the translated sentences into one coherent result per policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d73737-af7e-40d5-8f5a-432518adbefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Harish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f198b1a-05ad-4aa6-8f37-6576141e44cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a604b84e-425f-4d97-a493-e5fb64821f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_policy_mbart(policy_text, target_lang):\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    sentences = sent_tokenize(policy_text)\n",
    "    translated_sentences = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoded = tokenizer(sent, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        generated = model.generate(**encoded, forced_bos_token_id=tokenizer.lang_code_to_id[target_lang], max_length=512)\n",
    "        translated = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "        translated_sentences.append(translated)\n",
    "\n",
    "    return ' '.join(translated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b95e9bb-ff78-4ce1-bdab-c31a3ea78d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Policy_Text_FR_mBART\"] = df[\"Policy_Text_EN_Clean\"].apply(lambda x: translate_policy_mbart(x, \"fr_XX\"))\n",
    "df[\"Policy_Text_ES_mBART\"] = df[\"Policy_Text_EN_Clean\"].apply(lambda x: translate_policy_mbart(x, \"es_XX\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a294fa5f-325f-40af-ade2-7be4e35ce39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"./Data/translated_insurance_policies.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4053c9-a63b-4f26-848a-7db370515a5c",
   "metadata": {},
   "source": [
    "# Summarize Preprocessed Insurance Policy Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0c791b9-46a1-4649-a476-f55420c89eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Excel File\n",
    "#df = pd.read_excel(\"./Data/translated_insurance_policies.xlsx\")\n",
    "\n",
    "# Load summarization model (BART)\n",
    "summarizer_name = \"facebook/bart-large-cnn\"\n",
    "summarizer_tokenizer = BartTokenizer.from_pretrained(summarizer_name)\n",
    "summarizer_model = BartForConditionalGeneration.from_pretrained(summarizer_name).to(device)\n",
    "\n",
    "# Load translation model (mBART)\n",
    "translator_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "translator_tokenizer = MBart50TokenizerFast.from_pretrained(translator_name)\n",
    "translator_model = MBartForConditionalGeneration.from_pretrained(translator_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dde5cb87-3c72-4255-849d-b94728136754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization function\n",
    "def summarize_long_text(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    inputs = summarizer_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    summary_ids = summarizer_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        num_beams=4,\n",
    "        max_length= 520,\n",
    "        min_length=150,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5483c31-cc79-4e53-aed8-00e592e4c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation function\n",
    "def translate(text, target_lang=\"fr_XX\"):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    translator_tokenizer.src_lang = \"en_XX\"\n",
    "    encoded = translator_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    generated_tokens = translator_model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=translator_tokenizer.lang_code_to_id[target_lang],\n",
    "        max_length=512\n",
    "    )\n",
    "    return translator_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "846d39e6-d537-4a81-8c33-d8a3e6e28ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply summarization and translations\n",
    "summaries_en = []\n",
    "summaries_fr = []\n",
    "summaries_es = []\n",
    "\n",
    "for text in df[\"Policy_Text_EN_Clean\"]:\n",
    "    summary = summarize_long_text(text)\n",
    "    summaries_en.append(summary)\n",
    "    summaries_fr.append(translate(summary, target_lang=\"fr_XX\"))\n",
    "    summaries_es.append(translate(summary, target_lang=\"es_XX\"))\n",
    "\n",
    "# Add columns to DataFrame\n",
    "df[\"Policy_Summary_EN\"] = summaries_en\n",
    "df[\"Policy_Summary_FR\"] = summaries_fr\n",
    "df[\"Policy_Summary_ES\"] = summaries_es\n",
    "\n",
    "# Save to new Excel file\n",
    "output_file = \"translated_insurance_policies_with_summaries.xlsx\"\n",
    "df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964c0cc-002c-4ed0-b00a-fe7fcb332153",
   "metadata": {},
   "source": [
    "# Sentence Alignment Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec55b622-8d82-486e-9bd1-24505d4e2e4b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7932384-9d1b-450f-b886-91648ddf8c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf031bf6-7989-455a-97d9-95b90fd40329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/Harish/Desktop/GUVI/FinalProject_2/Data/translated_insurance_policies_with_summaries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9784edfa-60db-4fb6-afaf-2585d8cc179d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Policy_ID</th>\n",
       "      <th>Policy_Name</th>\n",
       "      <th>Policy_Text_EN</th>\n",
       "      <th>Policy_Text_EN_Clean</th>\n",
       "      <th>Policy_Text_FR_mBART</th>\n",
       "      <th>Policy_Text_ES_mBART</th>\n",
       "      <th>Policy_Summary_EN</th>\n",
       "      <th>Policy_Summary_FR</th>\n",
       "      <th>Policy_Summary_ES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P001</td>\n",
       "      <td>Bike Insurance</td>\n",
       "      <td>Bike insurance is the ultimate safety net for ...</td>\n",
       "      <td>bike insurance is the ultimate safety net for ...</td>\n",
       "      <td>L'assurance-bicyclette est le filet de sécurit...</td>\n",
       "      <td>la seguridad de la moto es la máxima seguridad...</td>\n",
       "      <td>Bike insurance provides coverage against natur...</td>\n",
       "      <td>L'assurance-bicyclette offre une protection co...</td>\n",
       "      <td>El seguro de bicicleta ofrece cobertura contra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P002</td>\n",
       "      <td>Car Insurance</td>\n",
       "      <td>Car insurance, also known as auto or motor ins...</td>\n",
       "      <td>car insurance, also known as auto or motor ins...</td>\n",
       "      <td>L'assurance automobile, également connue sous ...</td>\n",
       "      <td>el seguro de coche, también conocido como segu...</td>\n",
       "      <td>Car insurance is a financial safety net that p...</td>\n",
       "      <td>L'assurance automobile est un réseau de sécuri...</td>\n",
       "      <td>El seguro de coche es una red de seguridad fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P003</td>\n",
       "      <td>Health Insurance</td>\n",
       "      <td>Health insurance, also known as medical insura...</td>\n",
       "      <td>health insurance, also known as medical insura...</td>\n",
       "      <td>L'assurance maladie, également connue sous le ...</td>\n",
       "      <td>seguro médico, también conocido como seguro mé...</td>\n",
       "      <td>health insurance, also known as medical insura...</td>\n",
       "      <td>assurance maladie, également connue sous le no...</td>\n",
       "      <td>seguro médico, también conocido como seguro mé...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Policy_ID       Policy_Name  \\\n",
       "0      P001    Bike Insurance   \n",
       "1      P002     Car Insurance   \n",
       "2      P003  Health Insurance   \n",
       "\n",
       "                                      Policy_Text_EN  \\\n",
       "0  Bike insurance is the ultimate safety net for ...   \n",
       "1  Car insurance, also known as auto or motor ins...   \n",
       "2  Health insurance, also known as medical insura...   \n",
       "\n",
       "                                Policy_Text_EN_Clean  \\\n",
       "0  bike insurance is the ultimate safety net for ...   \n",
       "1  car insurance, also known as auto or motor ins...   \n",
       "2  health insurance, also known as medical insura...   \n",
       "\n",
       "                                Policy_Text_FR_mBART  \\\n",
       "0  L'assurance-bicyclette est le filet de sécurit...   \n",
       "1  L'assurance automobile, également connue sous ...   \n",
       "2  L'assurance maladie, également connue sous le ...   \n",
       "\n",
       "                                Policy_Text_ES_mBART  \\\n",
       "0  la seguridad de la moto es la máxima seguridad...   \n",
       "1  el seguro de coche, también conocido como segu...   \n",
       "2  seguro médico, también conocido como seguro mé...   \n",
       "\n",
       "                                   Policy_Summary_EN  \\\n",
       "0  Bike insurance provides coverage against natur...   \n",
       "1  Car insurance is a financial safety net that p...   \n",
       "2  health insurance, also known as medical insura...   \n",
       "\n",
       "                                   Policy_Summary_FR  \\\n",
       "0  L'assurance-bicyclette offre une protection co...   \n",
       "1  L'assurance automobile est un réseau de sécuri...   \n",
       "2  assurance maladie, également connue sous le no...   \n",
       "\n",
       "                                   Policy_Summary_ES  \n",
       "0  El seguro de bicicleta ofrece cobertura contra...  \n",
       "1  El seguro de coche es una red de seguridad fin...  \n",
       "2  seguro médico, también conocido como seguro mé...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562f1e24-7ac1-486d-b247-1bdddab8e1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting translation similarity analysis...\n",
      "Calculating similarity scores for each translation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:51<00:00,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRANSLATION SIMILARITY ANALYSIS RESULTS\n",
      "==================================================\n",
      "\n",
      "Total translations analyzed: 8\n",
      "\n",
      "Similarity Score Statistics:\n",
      "Metric                         Mean     Std      Min      Max     \n",
      "--------------------------------------------------------------\n",
      "cosine_similarity_mean_pooling 0.836    0.048    0.719    0.862   \n",
      "cosine_similarity_cls_pooling  0.901    0.103    0.648    0.954   \n",
      "token_alignment_score          0.597    0.037    0.524    0.644   \n",
      "\n",
      "Quality Assessment (based on mean pooling cosine similarity):\n",
      "High quality translations (>0.8): 7 (87.5%)\n",
      "Medium quality translations (0.6-0.8): 1 (12.5%)\n",
      "Low quality translations (≤0.6): 0 (0.0%)\n",
      "\n",
      "==================================================\n",
      "BEST TRANSLATIONS (Top 5)\n",
      "==================================================\n",
      "\n",
      "Similarity Score: 0.862\n",
      "Source: bike insurance is the ultimate safety net for your two-wheeler. be it a simple moped or a superbike,...\n",
      "Target: L'assurance-bicyclette est le filet de sécurité ultime pour votre bicyclette. qu'il s'agisse d'un si...\n",
      "----------------------------------------\n",
      "\n",
      "Similarity Score: 0.858\n",
      "Source: home insurance is a type of insurance that protects your home, its furnishings, and valuable items f...\n",
      "Target: L'assurance familiale est un type d'assurance qui protège votre maison, ses meubles et ses biens pré...\n",
      "----------------------------------------\n",
      "\n",
      "Similarity Score: 0.857\n",
      "Source: car insurance, also known as auto or motor insurance, is a financial safety net that protects you an...\n",
      "Target: L'assurance automobile, également connue sous le nom d'assurance automobile ou d'assurance automobil...\n",
      "----------------------------------------\n",
      "\n",
      "Similarity Score: 0.857\n",
      "Source: health insurance, also known as medical insurance, is your ultimate safety shield, safeguarding you ...\n",
      "Target: L'assurance maladie, également connue sous le nom d'assurance médicale, est votre dernier sceau de s...\n",
      "----------------------------------------\n",
      "\n",
      "Similarity Score: 0.852\n",
      "Source: commercial vehicle insurance is a mandatory motor insurance that covers your commercial vehicle agai...\n",
      "Target: l'assurance automobile est une assurance automobile obligatoire qui couvre votre véhicule commercial...\n",
      "----------------------------------------\n",
      "\n",
      "Sample Results:\n",
      "   index  cosine_similarity_mean_pooling  cosine_similarity_cls_pooling  \\\n",
      "0      0                        0.861502                       0.932195   \n",
      "1      1                        0.857333                       0.943634   \n",
      "2      2                        0.856656                       0.945485   \n",
      "3      3                        0.848467                       0.919053   \n",
      "4      4                        0.857518                       0.931089   \n",
      "5      5                        0.852216                       0.938197   \n",
      "6      6                        0.718718                       0.647689   \n",
      "7      7                        0.836275                       0.953542   \n",
      "\n",
      "   token_alignment_score  \n",
      "0               0.629302  \n",
      "1               0.643728  \n",
      "2               0.616990  \n",
      "3               0.581011  \n",
      "4               0.601019  \n",
      "5               0.595791  \n",
      "6               0.524479  \n",
      "7               0.583686  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\", output_hidden_states=True)\n",
    "\n",
    "def get_sentence_embedding(text, tokenizer, model, pooling='mean'):\n",
    "    \"\"\"\n",
    "    Get sentence-level embedding using different pooling strategies\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state.squeeze(0)\n",
    "        \n",
    "        # Remove [CLS] and [SEP] tokens for mean pooling\n",
    "        if pooling == 'mean':\n",
    "            # Exclude [CLS] (first) and [SEP] (last) tokens\n",
    "            token_embeddings = hidden_states[1:-1]\n",
    "            sentence_embedding = torch.mean(token_embeddings, dim=0)\n",
    "        elif pooling == 'cls':\n",
    "            # Use [CLS] token embedding\n",
    "            sentence_embedding = hidden_states[0]\n",
    "        elif pooling == 'max':\n",
    "            # Max pooling over token embeddings\n",
    "            token_embeddings = hidden_states[1:-1]\n",
    "            sentence_embedding = torch.max(token_embeddings, dim=0)[0]\n",
    "    \n",
    "    return sentence_embedding.numpy()\n",
    "\n",
    "def calculate_similarity_scores(df, src_col='Policy_Text_EN_Clean', tgt_col='Policy_Text_FR_mBART'):\n",
    "    \"\"\"\n",
    "    Calculate various similarity scores for each translation pair\n",
    "    \"\"\"\n",
    "    similarity_results = []\n",
    "    \n",
    "    print(\"Calculating similarity scores for each translation...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        sent_src = str(row[src_col])\n",
    "        sent_tgt = str(row[tgt_col])\n",
    "        \n",
    "        # Skip if either text is empty or NaN\n",
    "        if pd.isna(sent_src) or pd.isna(sent_tgt) or sent_src.strip() == '' or sent_tgt.strip() == '':\n",
    "            continue\n",
    "        \n",
    "        # Get sentence embeddings using different pooling methods\n",
    "        src_emb_mean = get_sentence_embedding(sent_src, tokenizer, model, pooling='mean')\n",
    "        tgt_emb_mean = get_sentence_embedding(sent_tgt, tokenizer, model, pooling='mean')\n",
    "        \n",
    "        src_emb_cls = get_sentence_embedding(sent_src, tokenizer, model, pooling='cls')\n",
    "        tgt_emb_cls = get_sentence_embedding(sent_tgt, tokenizer, model, pooling='cls')\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        cos_sim_mean = cosine_similarity([src_emb_mean], [tgt_emb_mean])[0][0]\n",
    "        cos_sim_cls = cosine_similarity([src_emb_cls], [tgt_emb_cls])[0][0]\n",
    "        \n",
    "        # Calculate token-level alignment score (your original approach)\n",
    "        token_alignment_score = calculate_token_alignment_score(sent_src, sent_tgt, tokenizer, model)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'index': idx,\n",
    "            'source_text': sent_src[:100] + '...' if len(sent_src) > 100 else sent_src,\n",
    "            'target_text': sent_tgt[:100] + '...' if len(sent_tgt) > 100 else sent_tgt,\n",
    "            'cosine_similarity_mean_pooling': cos_sim_mean,\n",
    "            'cosine_similarity_cls_pooling': cos_sim_cls,\n",
    "            'token_alignment_score': token_alignment_score,\n",
    "            'source_length': len(sent_src.split()),\n",
    "            'target_length': len(sent_tgt.split())\n",
    "        }\n",
    "        \n",
    "        similarity_results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(similarity_results)\n",
    "\n",
    "def calculate_token_alignment_score(sent_src, sent_tgt, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Calculate token-level alignment score (average of best matches)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Tokenize separately\n",
    "        src_inputs = tokenizer(\n",
    "            sent_src,\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        tgt_inputs = tokenizer(\n",
    "            sent_tgt,\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            src_outputs = model(**src_inputs)\n",
    "            tgt_outputs = model(**tgt_inputs)\n",
    "        \n",
    "        # Get token embeddings and remove [CLS] and [SEP]\n",
    "        src_emb = src_outputs.last_hidden_state.squeeze(0)[1:-1]\n",
    "        tgt_emb = tgt_outputs.last_hidden_state.squeeze(0)[1:-1]\n",
    "        \n",
    "        if src_emb.size(0) == 0 or tgt_emb.size(0) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Compute cosine similarity matrix\n",
    "        similarity_matrix = torch.nn.functional.cosine_similarity(\n",
    "            src_emb.unsqueeze(1),\n",
    "            tgt_emb.unsqueeze(0),\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        # Get best alignment score for each source token\n",
    "        best_scores = torch.max(similarity_matrix, dim=1)[0]\n",
    "        avg_alignment_score = torch.mean(best_scores).item()\n",
    "        \n",
    "        return avg_alignment_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating token alignment: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def analyze_similarity_results(results_df):\n",
    "    \"\"\"\n",
    "    Analyze and visualize similarity results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRANSLATION SIMILARITY ANALYSIS RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nTotal translations analyzed: {len(results_df)}\")\n",
    "    print(f\"\\nSimilarity Score Statistics:\")\n",
    "    print(f\"{'Metric':<30} {'Mean':<8} {'Std':<8} {'Min':<8} {'Max':<8}\")\n",
    "    print(\"-\" * 62)\n",
    "    \n",
    "    metrics = ['cosine_similarity_mean_pooling', 'cosine_similarity_cls_pooling', 'token_alignment_score']\n",
    "    for metric in metrics:\n",
    "        mean_val = results_df[metric].mean()\n",
    "        std_val = results_df[metric].std()\n",
    "        min_val = results_df[metric].min()\n",
    "        max_val = results_df[metric].max()\n",
    "        print(f\"{metric:<30} {mean_val:<8.3f} {std_val:<8.3f} {min_val:<8.3f} {max_val:<8.3f}\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    print(f\"\\nQuality Assessment (based on mean pooling cosine similarity):\")\n",
    "    high_quality = len(results_df[results_df['cosine_similarity_mean_pooling'] > 0.8])\n",
    "    medium_quality = len(results_df[(results_df['cosine_similarity_mean_pooling'] > 0.6) & \n",
    "                                   (results_df['cosine_similarity_mean_pooling'] <= 0.8)])\n",
    "    low_quality = len(results_df[results_df['cosine_similarity_mean_pooling'] <= 0.6])\n",
    "    \n",
    "    print(f\"High quality translations (>0.8): {high_quality} ({high_quality/len(results_df)*100:.1f}%)\")\n",
    "    print(f\"Medium quality translations (0.6-0.8): {medium_quality} ({medium_quality/len(results_df)*100:.1f}%)\")\n",
    "    print(f\"Low quality translations (≤0.6): {low_quality} ({low_quality/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Show best examples\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"BEST TRANSLATIONS (Top 5)\")\n",
    "    print(\"=\"*50)\n",
    "    best_translations = results_df.nlargest(5, 'cosine_similarity_mean_pooling')\n",
    "    for _, row in best_translations.iterrows():\n",
    "        print(f\"\\nSimilarity Score: {row['cosine_similarity_mean_pooling']:.3f}\")\n",
    "        print(f\"Source: {row['source_text']}\")\n",
    "        print(f\"Target: {row['target_text']}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Starting translation similarity analysis...\")\n",
    "    \n",
    "    # Calculate similarity scores for all translations\n",
    "    similarity_results = calculate_similarity_scores(df)\n",
    "    \n",
    "    # Analyze results\n",
    "    analyzed_results = analyze_similarity_results(similarity_results)\n",
    "    \n",
    "    # Display sample results\n",
    "    print(f\"\\nSample Results:\")\n",
    "    print(similarity_results[['index', 'cosine_similarity_mean_pooling', \n",
    "                             'cosine_similarity_cls_pooling', 'token_alignment_score']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2267e8b-9d5f-471a-a4c7-0fda81193023",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b859865-e93c-4c31-80f8-a981fb249b1f",
   "metadata": {},
   "source": [
    "### Train Hugging Face Transformer model mBART using this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b89f43-3425-41e9-81a8-8a359a1f8a6c",
   "metadata": {},
   "source": [
    "### Train and Save French Translator model mBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa0d7ff0-3396-4e02-9b50-d3687a6bb0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 57.75 examples/s]\n",
      "C:\\Users\\Harish\\AppData\\Local\\Temp\\ipykernel_1312\\748911723.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 05:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=2.76182222366333, metrics={'train_runtime': 474.9411, 'train_samples_per_second': 0.017, 'train_steps_per_second': 0.008, 'total_flos': 8668519071744.0, 'train_loss': 2.76182222366333, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load data\n",
    "df = pd.read_excel(\"C:/Users/Harish/Desktop/GUVI/FinalProject_2/Data/translated_insurance_policies_with_summaries.xlsx\")\n",
    "df = df[['Policy_Text_EN_Clean', 'Policy_Text_FR_mBART']]\n",
    "df.columns = ['src_text', 'tgt_text']\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Step 2: Load model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"fr_XX\"\n",
    "\n",
    "# Step 3: Preprocess the data\n",
    "def preprocess(data):\n",
    "    return tokenizer(\n",
    "        data['src_text'],\n",
    "        text_target=data['tgt_text'],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Step 4: Load model\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Step 5: Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mbart-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"no\",  # no intermediate checkpoints\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Step 6: Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Step 7: Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a909db2-3a47-473c-a80c-97a970c60a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/tokenizer_config.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/special_tokens_map.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/sentencepiece.bpe.model',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac421272-3c5c-4a4c-9e78-da8a6d32a0f2",
   "metadata": {},
   "source": [
    "### Train and Save Spanish Translator model - mBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a683e221-182e-421d-a4ff-5bcb7d122104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 38.08 examples/s]\n",
      "C:\\Users\\Harish\\AppData\\Local\\Temp\\ipykernel_1312\\3143027975.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=5.286947250366211, metrics={'train_runtime': 338.9393, 'train_samples_per_second': 0.024, 'train_steps_per_second': 0.003, 'total_flos': 8668519071744.0, 'train_loss': 5.286947250366211, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load data\n",
    "df = pd.read_excel(\"C:/Users/Harish/Desktop/GUVI/FinalProject_2/Data/translated_insurance_policies_with_summaries.xlsx\")\n",
    "df = df[['Policy_Text_EN_Clean', 'Policy_Text_ES_mBART']]\n",
    "df.columns = ['src_text', 'tgt_text']\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Step 2: Load model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"es_XX\"\n",
    "\n",
    "# Step 3: Preprocess the data\n",
    "def preprocess(data):\n",
    "    return tokenizer(\n",
    "        data['src_text'],\n",
    "        text_target=data['tgt_text'],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Step 4: Load model\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Step 5: Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mbart-finetuned\",\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"no\",  # no intermediate checkpoints\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Step 6: Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Step 7: Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12d0e3ec-7962-4456-958a-35355a2a5239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/tokenizer_config.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/special_tokens_map.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/sentencepiece.bpe.model',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/added_tokens.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41328592-ba84-471b-8012-947c60daacd2",
   "metadata": {},
   "source": [
    "### Train and Save English Language Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6c0e504-bf56-4df7-86a8-d951fa5c4072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Policy_ID</th>\n",
       "      <th>Policy_Name</th>\n",
       "      <th>Policy_Text_EN</th>\n",
       "      <th>Policy_Text_EN_Clean</th>\n",
       "      <th>Policy_Text_FR_mBART</th>\n",
       "      <th>Policy_Text_ES_mBART</th>\n",
       "      <th>Policy_Summary_EN</th>\n",
       "      <th>Policy_Summary_FR</th>\n",
       "      <th>Policy_Summary_ES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P001</td>\n",
       "      <td>Bike Insurance</td>\n",
       "      <td>Bike insurance is the ultimate safety net for ...</td>\n",
       "      <td>bike insurance is the ultimate safety net for ...</td>\n",
       "      <td>L'assurance-bicyclette est le filet de sécurit...</td>\n",
       "      <td>la seguridad de la moto es la máxima seguridad...</td>\n",
       "      <td>Bike insurance provides coverage against natur...</td>\n",
       "      <td>L'assurance-bicyclette offre une protection co...</td>\n",
       "      <td>El seguro de bicicleta ofrece cobertura contra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Policy_ID     Policy_Name  \\\n",
       "0      P001  Bike Insurance   \n",
       "\n",
       "                                      Policy_Text_EN  \\\n",
       "0  Bike insurance is the ultimate safety net for ...   \n",
       "\n",
       "                                Policy_Text_EN_Clean  \\\n",
       "0  bike insurance is the ultimate safety net for ...   \n",
       "\n",
       "                                Policy_Text_FR_mBART  \\\n",
       "0  L'assurance-bicyclette est le filet de sécurit...   \n",
       "\n",
       "                                Policy_Text_ES_mBART  \\\n",
       "0  la seguridad de la moto es la máxima seguridad...   \n",
       "\n",
       "                                   Policy_Summary_EN  \\\n",
       "0  Bike insurance provides coverage against natur...   \n",
       "\n",
       "                                   Policy_Summary_FR  \\\n",
       "0  L'assurance-bicyclette offre une protection co...   \n",
       "\n",
       "                                   Policy_Summary_ES  \n",
       "0  El seguro de bicicleta ofrece cobertura contra...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"C:/Users/Harish/Desktop/GUVI/FinalProject_2/Data/translated_insurance_policies_with_summaries.xlsx\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca1d59ab-6d59-463f-b98c-8340f505648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Policy_Text_EN_Clean', 'Policy_Summary_EN']]\n",
    "df.columns = ['src_text', 'tgt_text']\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d3bdd61-82fd-46eb-bd5a-3f55e82ec2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "170aa091-6e1f-4bd6-a203-c567e628d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def preprocess(data):\n",
    "    return tokenizer(\n",
    "        data['src_text'],\n",
    "        text_target=data['tgt_text'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e903a627-1197-461f-85b8-4606dfbedac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8/8 [00:00<00:00, 30.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "646c79f2-6982-44cf-ad93-53ea0812a02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\AppData\\Local\\Temp\\ipykernel_1312\\987811970.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 09:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/summarizer/tokenizer_config.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/summarizer/special_tokens_map.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/summarizer/vocab.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/summarizer/merges.txt',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/summarizer/added_tokens.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart-summary-model\",\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save model and tokenizer for Streamlit app\n",
    "save_path = \"C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/summarizer/\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c85a3a-b594-41a0-b69b-4786d6683636",
   "metadata": {},
   "source": [
    "### Train and Save French Translator model - Helsinki-NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a9d5ef9-05fa-4a21-800f-e6e6529bc4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Harish\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6623505-25c0-446d-9bd6-b4c3be47eef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Policy_ID</th>\n",
       "      <th>Policy_Name</th>\n",
       "      <th>Policy_Text_EN</th>\n",
       "      <th>Policy_Text_EN_Clean</th>\n",
       "      <th>Policy_Text_FR_mBART</th>\n",
       "      <th>Policy_Text_ES_mBART</th>\n",
       "      <th>Policy_Summary_EN</th>\n",
       "      <th>Policy_Summary_FR</th>\n",
       "      <th>Policy_Summary_ES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P001</td>\n",
       "      <td>Bike Insurance</td>\n",
       "      <td>Bike insurance is the ultimate safety net for ...</td>\n",
       "      <td>bike insurance is the ultimate safety net for ...</td>\n",
       "      <td>L'assurance-bicyclette est le filet de sécurit...</td>\n",
       "      <td>la seguridad de la moto es la máxima seguridad...</td>\n",
       "      <td>Bike insurance provides coverage against natur...</td>\n",
       "      <td>L'assurance-bicyclette offre une protection co...</td>\n",
       "      <td>El seguro de bicicleta ofrece cobertura contra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Policy_ID     Policy_Name  \\\n",
       "0      P001  Bike Insurance   \n",
       "\n",
       "                                      Policy_Text_EN  \\\n",
       "0  Bike insurance is the ultimate safety net for ...   \n",
       "\n",
       "                                Policy_Text_EN_Clean  \\\n",
       "0  bike insurance is the ultimate safety net for ...   \n",
       "\n",
       "                                Policy_Text_FR_mBART  \\\n",
       "0  L'assurance-bicyclette est le filet de sécurit...   \n",
       "\n",
       "                                Policy_Text_ES_mBART  \\\n",
       "0  la seguridad de la moto es la máxima seguridad...   \n",
       "\n",
       "                                   Policy_Summary_EN  \\\n",
       "0  Bike insurance provides coverage against natur...   \n",
       "\n",
       "                                   Policy_Summary_FR  \\\n",
       "0  L'assurance-bicyclette offre une protection co...   \n",
       "\n",
       "                                   Policy_Summary_ES  \n",
       "0  El seguro de bicicleta ofrece cobertura contra...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "df = pd.read_excel(\"C:/Users/Harish/Desktop/GUVI/FinalProject_2/Data/translated_insurance_policies_with_summaries.xlsx\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7736a4d3-74a7-4f3f-a0ae-1eac0a7ca2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the columns for training translation\n",
    "df = df[[\"Policy_Text_EN_Clean\", \"Policy_Text_FR_mBART\"]]\n",
    "df = df.rename(columns={\n",
    "    \"Policy_Text_EN_Clean\": \"src_text\",\n",
    "    \"Policy_Text_FR_mBART\": \"tgt_text\"\n",
    "})\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbdcdce9-9d5e-4d10-a226-0c36dce94523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 39.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess function\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"src_text\"], max_length=256, padding=\"max_length\", truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        targets = tokenizer(examples[\"tgt_text\"], max_length=256, padding=\"max_length\", truncation=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "372f4725-b790-497b-ac78-978b2ff92a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\AppData\\Local\\Temp\\ipykernel_1312\\2413807225.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.8785576820373535, metrics={'train_runtime': 38.5505, 'train_samples_per_second': 0.623, 'train_steps_per_second': 0.078, 'total_flos': 1627121516544.0, 'train_loss': 0.8785576820373535, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./marian-en-fr-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3768853-1189-49fe-b3dd-710f91a8abdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/tokenizer_config.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/special_tokens_map.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/vocab.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/source.spm',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/target.spm',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/added_tokens.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer \n",
    "save_path = \"C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/frenchtranslator/\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b10e182-1680-4dba-bf87-6ed3fbc762fc",
   "metadata": {},
   "source": [
    "### Train and Save the Spanish Translator - Helsinki-NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e35d28d-bdf2-47fa-93ae-735987f89a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Policy_ID</th>\n",
       "      <th>Policy_Name</th>\n",
       "      <th>Policy_Text_EN</th>\n",
       "      <th>Policy_Text_EN_Clean</th>\n",
       "      <th>Policy_Text_FR_mBART</th>\n",
       "      <th>Policy_Text_ES_mBART</th>\n",
       "      <th>Policy_Summary_EN</th>\n",
       "      <th>Policy_Summary_FR</th>\n",
       "      <th>Policy_Summary_ES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P001</td>\n",
       "      <td>Bike Insurance</td>\n",
       "      <td>Bike insurance is the ultimate safety net for ...</td>\n",
       "      <td>bike insurance is the ultimate safety net for ...</td>\n",
       "      <td>L'assurance-bicyclette est le filet de sécurit...</td>\n",
       "      <td>la seguridad de la moto es la máxima seguridad...</td>\n",
       "      <td>Bike insurance provides coverage against natur...</td>\n",
       "      <td>L'assurance-bicyclette offre une protection co...</td>\n",
       "      <td>El seguro de bicicleta ofrece cobertura contra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Policy_ID     Policy_Name  \\\n",
       "0      P001  Bike Insurance   \n",
       "\n",
       "                                      Policy_Text_EN  \\\n",
       "0  Bike insurance is the ultimate safety net for ...   \n",
       "\n",
       "                                Policy_Text_EN_Clean  \\\n",
       "0  bike insurance is the ultimate safety net for ...   \n",
       "\n",
       "                                Policy_Text_FR_mBART  \\\n",
       "0  L'assurance-bicyclette est le filet de sécurit...   \n",
       "\n",
       "                                Policy_Text_ES_mBART  \\\n",
       "0  la seguridad de la moto es la máxima seguridad...   \n",
       "\n",
       "                                   Policy_Summary_EN  \\\n",
       "0  Bike insurance provides coverage against natur...   \n",
       "\n",
       "                                   Policy_Summary_FR  \\\n",
       "0  L'assurance-bicyclette offre une protection co...   \n",
       "\n",
       "                                   Policy_Summary_ES  \n",
       "0  El seguro de bicicleta ofrece cobertura contra...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"C:/Users/Harish/Desktop/GUVI/FinalProject_2/Data/translated_insurance_policies_with_summaries.xlsx\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "404714fc-0eb5-4f83-a202-aa4a22b7059d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Harish\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-es. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c027808-dd68-47bf-83e5-1403ad256eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 98.45 examples/s]\n",
      "C:\\Users\\Harish\\AppData\\Local\\Temp\\ipykernel_1312\\1803436984.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\Harish\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.7281883557637533, metrics={'train_runtime': 35.1041, 'train_samples_per_second': 0.684, 'train_steps_per_second': 0.085, 'total_flos': 1627121516544.0, 'train_loss': 0.7281883557637533, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only the columns for training translation\n",
    "df = df[[\"Policy_Text_EN_Clean\", \"Policy_Text_ES_mBART\"]]\n",
    "df = df.rename(columns={\n",
    "    \"Policy_Text_EN_Clean\": \"src_text\",\n",
    "    \"Policy_Text_ES_mBART\": \"tgt_text\"\n",
    "})\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_function(data):\n",
    "    inputs = tokenizer(data[\"src_text\"], max_length=256, padding=\"max_length\", truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        targets = tokenizer(data[\"tgt_text\"], max_length=256, padding=\"max_length\", truncation=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./marian-en-es-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b49c1d2d-3706-46fd-bd61-784d2287cd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/tokenizer_config.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/special_tokens_map.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/vocab.json',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/source.spm',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/target.spm',\n",
       " 'C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/added_tokens.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer \n",
    "save_path = \"C:/Users/Harish/Desktop/GUVI/FinalProject_2/artifacts/spanishtranslator/\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfcca18-79e8-4cce-bf57-cfc7c154159f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
